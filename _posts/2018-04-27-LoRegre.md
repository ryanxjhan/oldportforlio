---
layout: mypost
title: Loyalty Forcasting by Logistic Regression
categories: [Regression]
---

```python
import numpy as np 
import pandas as pd 
from patsy import dmatrices 
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split, cross_val_score 
from sklearn import metrics 
import matplotlib.pyplot as plt 
```

### Read Data


```python
data = pd.read_csv("HR_comma_sep.csv")
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>satisfaction_level</th>
      <th>last_evaluation</th>
      <th>number_project</th>
      <th>average_montly_hours</th>
      <th>time_spend_company</th>
      <th>Work_accident</th>
      <th>left</th>
      <th>promotion_last_5years</th>
      <th>sales</th>
      <th>salary</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.38</td>
      <td>0.53</td>
      <td>2</td>
      <td>157</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>sales</td>
      <td>low</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.80</td>
      <td>0.86</td>
      <td>5</td>
      <td>262</td>
      <td>6</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>sales</td>
      <td>medium</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.11</td>
      <td>0.88</td>
      <td>7</td>
      <td>272</td>
      <td>4</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>sales</td>
      <td>medium</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.72</td>
      <td>0.87</td>
      <td>5</td>
      <td>223</td>
      <td>5</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>sales</td>
      <td>low</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.37</td>
      <td>0.52</td>
      <td>2</td>
      <td>159</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>sales</td>
      <td>low</td>
    </tr>
  </tbody>
</table>
</div>




```python
data.dtypes
```




    satisfaction_level       float64
    last_evaluation          float64
    number_project             int64
    average_montly_hours       int64
    time_spend_company         int64
    Work_accident              int64
    left                       int64
    promotion_last_5years      int64
    sales                     object
    salary                    object
    dtype: object



### Salary


```python
pd.crosstab(data.salary, data.left).plot(kind='bar')
plt.show()
```


![png](kernel2_5_0.png)



```python
q = pd.crosstab(data.salary, data.left)
print(q)
print(q.sum(1))
q.div(q.sum(1), axis = 0).plot(kind='bar', stacked = True)
plt.show()
```

    left       0     1
    salary            
    high    1155    82
    low     5144  2172
    medium  5129  1317
    salary
    high      1237
    low       7316
    medium    6446
    dtype: int64



![png](kernel2_6_1.png)


### Satisfaction


```python
data[data.left==0].satisfaction_level.hist()
plt.show()
```


![png](kernel2_8_0.png)



```python
data[data.left==1].satisfaction_level.hist()
plt.show()
```


![png](kernel2_9_0.png)


### One-hot Encoding and Renaming
* dmatrices


```python
model = LogisticRegression()
y, X = dmatrices('left~satisfaction_level+last_evaluation+number_project+average_montly_hours+time_spend_company+Work_accident+promotion_last_5years+C(sales)+C(salary)', data, return_type='dataframe')
X = X.rename(columns = {
    'C(sales)[T.RandD]': 'Department: Random',
    'C(sales)[T.accounting]': 'Department: Accounting',
    'C(sales)[T.hr]': 'Department: HR',
    'C(sales)[T.management]': 'Department: Management',
    'C(sales)[T.marketing]': 'Department: Marketing',
    'C(sales)[T.product_mng]': 'Department: Product_Management',
    'C(sales)[T.sales]': 'Department: Sales',
    'C(sales)[T.support]': 'Department: Support',
    'C(sales)[T.technical]': 'Department: Technical',
    'C(salary)[T.low]': 'Salary: Low',
    'C(salary)[T.medium]': 'Salary: Medium'}) 
y = np.ravel(y) # Turn into one-dimensional array
```

### Model
* zip


```python
model.fit(X, y)
pd.DataFrame(list(zip(X.columns, np.transpose(model.coef_))))
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Intercept</td>
      <td>[-0.7241218101303709]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Department: Random</td>
      <td>[-0.4646533540507806]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Department: Accounting</td>
      <td>[0.11480746995707178]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Department: HR</td>
      <td>[0.34470249740075476]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Department: Management</td>
      <td>[-0.36197691576765323]</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Department: Marketing</td>
      <td>[0.0942542721870518]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Department: Product_Management</td>
      <td>[-0.03808031714105212]</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Department: Sales</td>
      <td>[0.06953539021768723]</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Department: Support</td>
      <td>[0.15060969294430204]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Department: Technical</td>
      <td>[0.1736770340382351]</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Salary: Low</td>
      <td>[1.785248331168715]</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Salary: Medium</td>
      <td>[1.2554599143449081]</td>
    </tr>
    <tr>
      <th>12</th>
      <td>satisfaction_level</td>
      <td>[-4.0764127612514125]</td>
    </tr>
    <tr>
      <th>13</th>
      <td>last_evaluation</td>
      <td>[0.7245059651665638]</td>
    </tr>
    <tr>
      <th>14</th>
      <td>number_project</td>
      <td>[-0.3098188001284907]</td>
    </tr>
    <tr>
      <th>15</th>
      <td>average_montly_hours</td>
      <td>[0.004362368915625797]</td>
    </tr>
    <tr>
      <th>16</th>
      <td>time_spend_company</td>
      <td>[0.2631525369965279]</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Work_accident</td>
      <td>[-1.5054703232132023]</td>
    </tr>
    <tr>
      <th>18</th>
      <td>promotion_last_5years</td>
      <td>[-1.1290360281564082]</td>
    </tr>
  </tbody>
</table>
</div>



### Score


```python
print(model.score(X,y))
```

    0.7919194612974199


### Test
* high salary, HR, satisfaction: 0.5, evaluation: 0.7, 4 projects, 160 hours, 3 years, no promotion, no work injuries


```python
model.predict_proba([[1,0,0,1,0,0,0,0,0,0,0,0, 0.5, 0.7, 4.0, 160, 3.0, 0, 0]])
```




    array([[0.9157868, 0.0842132]])




```python
pred = model.predict(X)
(abs(pred-y)).sum() / len(y)
```




    0.20808053870258017



### Split Data


```python
Xtrain,Xtest,ytrain,ytest=train_test_split(X, y, test_size=0.3, random_state=0)
model2 = LogisticRegression()
model2.fit(Xtrain, ytrain)
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
              verbose=0, warm_start=False)



### Traning Data 


```python
model2 = LogisticRegression(C=10000)
model2.fit(Xtrain, ytrain)
pred = model2.predict(Xtest)
metrics.accuracy_score(ytest, pred)
```




    0.7917777777777778



### Cofusion Matrix


```python
metrics.confusion_matrix(ytest, pred)
```




    array([[3199,  263],
           [ 674,  364]])



### Classfication Reprort


```python
#sklearn.metrics.classification_report(y_true, y_pred)
print(metrics.classification_report(ytest, pred))

```

                 precision    recall  f1-score   support
    
            0.0       0.83      0.92      0.87      3462
            1.0       0.58      0.35      0.44      1038
    
    avg / total       0.77      0.79      0.77      4500
    


### 10-fold Cross Validation


```python
print(cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10))
```

    [0.80679547 0.792      0.79533333 0.78733333 0.804      0.804
     0.794      0.79       0.74449633 0.73582388]

### Link
[Github](https://github.com/ryanxjhan/ml-practice/blob/master/HR%20Logistic%20Regression.ipynb)
